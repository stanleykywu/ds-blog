{
  
    
        "post0": {
            "title": "Stocks, Predictable or Unpredictable?",
            "content": "Usually I would follow what every english instructor in my life has told me: lead with a concise intro. But being the rebellious compsci major I am who has tried to escape all things english related (and failed miserably), I’m going to do something completely cliché to begin my first blog post and write a traditional introduction (thank you english teachers for the many years of practice). This is about a project that a group of friends and I did for our first “real” artificial intelligence class of college. To the fellow NEU students out there, the class is CS4100 and if he’s still teaching, I highly recommend Professor Kevin Gold. . . Is the Stock Market Predictable? . Seems like a relatively straight forward question, and for anyone who’s even remotely breathed in the direction of data science, this is (unsurprisingly) something a lot of people have attempted to answer. But with the latest developments in deep learning, this seems to be something that is potentially in reach. But there’s something different about predicting the stock market, vs. classifying images of cars and bikes for example. In the latter, image classification is a problem that we have manually defined as a challenge for machine learning. We have carefully and manually curated a dataset of images specifically for the purpose of image classifications. And while there is absolutely nothing wrong with that (image classification has become an incredible powerful and important tool of the present), it makes predicting the stock market rather hard to put into … should I say arrays? Maybe at first glance this might seem easy, and it could be. There’s a lot of numbers in stock market information, so shouldn’t that be enough? Well, while the numbers themselves offer some sort of reflection of the stock’s current state, it is so compressed and filtered value that one can argue it offers no value at all whatsoever. Something like news sentiment (whether or not the news that day was positive/negative) could offer a better insight into a stock market price’s trend. Regardless, there’s a massive amount of precious data out there that can potentially help predict stock market prices. But, Stanley, what are you trying to say with all of this? Fine, I’ll oblige: This blog is here to present how we were able to effectively predict Google’s stock market using three simple algorithms, albeit on a very small scale . . Alphabet Inc. Class C . In total, Kaggle was able to give us around 500 days worth of Google’s stock market price data that overlapped with news sentiment data we were able to pull from OpenML. That’s already a pretty small dataset, but we also maintained a roughly 60/20/20 train/val/test split, leaving us with roughly 300 consecutive training days, followed by 100 validation and 100 testing days. Even with this limited data, we were able to make some rather incredible discoveries. . . Maybe Stocks are Easy? . We decided to go with three different machine learning algorithms, a classic LSTM (Long Short-term Memory), a Regression Decision Tree using Residual Squared Sums, and an HMM (Hidden Markov Model). The LSTM represented a standard time series forecasting RNN, while the decision tree and HMM were attempts to apply slighly different algorithms to the task of stock market prediction. Both the LSTM and decision tree were trained on data with a “look-back” of one day. That means the prediction of stock prices for the next day was mainly based on that of the previous day. We found that this approach gave us the best validation performance. The results on testing data are as followed: . . . . What do We Make of this? . Immediately we can see that despite this lack of data, our three algorithms were able to do rather well on data it has never seen before. LSTM gives us consistently solid predictions, with a slight time lag that is consistent with the byproduct of LSTM’s “look-back” training data set. While at first glance, this seems moot, it is very important since day to day stock trading relies on accurate predictions for each specific day. On the other hand, decision trees seem to predict trends slightly before they occur (this is exciting!) due to its blockiness, likely a product of averaging done in the decision tree regressor. Finally for HMM, we can note that while the general trend seems similar, the HMM output tends to overshoot the actual value. This can be attributed to the fact that in the HMM model we used, the closing price tends to skew our predictions higher, which also results in a higher middle price (the value we are matching our predictions to). There also doesn’t seem to be an improved effectiveness of using news sentiment vs. not. Our best guess is that since we are looking at such a small scale (just about a year and half worth of data), that the stock market numbers are more than adequate to make one day decisions on a stock’s overall trend. . . Predicting the stock market may seem like a task perfectly designed for modern day heavy duty neural networks. Yet we show here that in smaller scales with publicly accessible data, a simpler approach may be more than adequate. While these algorithms probably won’t make you Jeff Bezos, it certainly shows that the stock market may not be the unpredictable monster that we used to think. For a more in depth read into this project, feel free to check out the repository on GitHub. .",
            "url": "https://stanleykywu.github.io/ds-blog/machine%20learning/stock%20market/decision%20trees/lstm/hmm/2020/12/14/stonks.html",
            "relUrl": "/machine%20learning/stock%20market/decision%20trees/lstm/hmm/2020/12/14/stonks.html",
            "date": " • Dec 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". More about me . I’m a current Computer Science major @ Northeastern University with a concentration in artificial intelligence. I aspire to be a machine learning researcher or data scientist, and I’m actively involved in adversarial machine learning research. . Hobbies: . Badminton | Classical Music | Boston Sports (Go Red Sox and Patriots!) | Hiking | Discussing Academic Papers (An Image is Worth 16x16 Words anyone?) | Video Games | . While I was writing this, I thought it might be cool to write like a conversation. That way, it’s more fun for me to write and hopefully more fun for you to read! Diving right into my hobbies: I love badminton, been playing since I was a kid (subtle flex but I was once top 20 in the Under 17 Category for Singles in the USA!). Here’s a picture of me at a college badminton tournament. . . I also played the violin in high school, where I got very interested in classical music. Nothing beats playing with an orchestra at Symphony Hall! Some of my favorite composers are Mahler, Beethoven, and Dvorak. Recently, I’ve also realized just how fun and amazing hiking is. Below is a picture of me at Mount Monadnock! . .",
          "url": "https://stanleykywu.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stanleykywu.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}