{
  
    
        "post0": {
            "title": "A Day in the Life of a Computer Science Researcher ft. Matthew Jagielski",
            "content": "from IPython.display import Audio from IPython.core.display import display display(Audio(&#39;Adversarial ML Research Podcast feat. Matthew Jagielski.mp3&#39;)) . Your browser does not support the audio element.",
            "url": "https://stanleykywu.github.io/ds-blog/2021/12/05/Day-in-the-Life-of-a-Computer-Science-Researcher-ft.-Matthew-Jagielski.html",
            "relUrl": "/2021/12/05/Day-in-the-Life-of-a-Computer-Science-Researcher-ft.-Matthew-Jagielski.html",
            "date": " • Dec 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "The Potential Threat Of Adversarial Machine Learning In The Software Industry",
            "content": "The Potential Threat of Adversarial Machine Learning in the Software Industry . Question: Given machine learning’s rapid incorporation into technology in the last ten years, does adversarial machine learning (maliciously attacking machine learning models to influence performance) pose a threat to the software development industry? . Purpose: To explore state of the art research and applications of adversarial machine learning and identify whether companies that depend/use machine learning are adequately prepared to protect their machine learning models against such attacks. . Machine learning has quickly become a popular “buzzword” for companies seeking to update their technology stacks with the state of the art. Yet unlike more traditional technology frameworks such as web development, and mobile app development … etc., that are mostly deterministic and easily tested, machine learning throws a wrench into what’s considered traditional software development. Machine learning is a flexible algorithm that learns and identifies patterns in datasets to automate decision making. Examples of machine learning applications include computer vision and natural language processing. Tesla’s autopilot feature uses computer vision, or CV for short, to analyze images of the road and tell the vehicle what to do, and there are many online chatbots that use natural language processing, or NLP, to respond to text queries. Flexibility is the greatest strength of machine learning, as well as its most dangerous feature. These models are prone to outside influence, which can cause harm when dealing with sensitive information. However this problem has not slipped under the noses of machine learning researchers. Between 2017 and 2019, there have been over 200 written papers on the potential pitfalls and failures of machine learning when attacked by an adversary (Marshall et al., 2018). Since academic research on attacking machine learning models, or adversarial machine learning, continues to be a focal point of machine learning and security research, the threat it poses to software development is also ever evolving. . Adversarial Example Attacks . Adversarial example attacks can cause machine learning models to make wrong decisions by slightly altering the input to the algorithm. Tesla’s self-driving car has been shown to fall prey to such adversarial examples when its Autopilot ECU, or electronic control unit, is compromised by an adversary (Tencent Keen Security Lab, 2019). These CV models take images as input, and evaluate the pixels before making a prediction on the corresponding image. Adversarial example attacks add specific perturbations to these images in order to fool the machine learning model to make a mistake in its prediction (Goodfellow, 2020). In extensive security research of the Tesla autopilot system, Tencent was able to successfully trick the computer vision modules of the windshield wiper and lane detection algorithms using adversarial examples. The research group successfully manipulated the fish eye camera image input, causing the CV algorithm to believe it was raining even when the image had no visible precipitation (Tencent Keen Security Lab, 2019). These researchers were also able to manipulate camera images of lanes and cause the CV algorithm to miss lane markings. However they concluded that this would be difficult to deploy, citing the robustness of the CV algorithm likely due to a very extensive training dataset (Tencent Keen Security Lab, 2019). Tencent’s attempt to use adversarial examples to manipulate Tesla’s autopilot was shown to be successful in precipitation detection, and less successful in lane detection, yet both offer insight into the potential threat and implications of adversarial attacks in a modern software setting. . Adversarial examples of CV models are not limited to perturbation of input images. A paper authored by researchers from Carnegie Mellon University and University of North Carolina show that it is possible to physically alter an individual’s appearance (in this case, with specialized glasses), to fool a CV model’s facial recognition algorithm (Sharif et al., 2017). This can present itself as a dangerous problem, especially when facial recognition is used for security purposes such as with air travel (Sharif et al., 2017). This attack does not require the same access to the models as with Tesla’s autopilot, instead introducing the possibility of fooling these computer vision models with physically created perturbations. Regardless, both cases of adversarial example attacks show that it is possible to manipulate computer vision models, which are commonly used in the software industry, by feeding intentionally perturbed examples to confuse the algorithms and lead them to make wrong predictions. . Poisoning Attacks . Poisoning attacks are a different kind of adversarial machine learning attack that has also shown to influence common machine learning models. This attack provides a model in training with data that is purposefully not indicative of the actual distribution, causing the model to make inaccurate predictions. Unlike adversarial example attacks, this approach is more theoretical, but still demonstrates how commonly used machine learning algorithms can be heavily influenced by adversaries with the right knowledge of how to do so. Researchers from the University of California, San Diego, were able to theoretically and experimentally show that a constantly updated machine learning model can be influenced over time by injecting specially designed malicious data points. This includes flipped labels of digit classification and slightly modifying data points that the model struggles the most with (Wang &amp; Chaudhuri, 2018). While this paper’s argument focuses on mathematical and theoretical arguments with some empirical results, it does not address the concern it raises in an industry setting, instead indicating more research is needed to investigate the potential vulnerabilities of such an attack. . The adversarial machine learning field is littered with theoretical papers, but the effects of a poisoning attack have also been seen in an industry setting. Microsoft’s Tay Chatbot was released in 2017, designed to learn and perform online interactions through Twitter tweets (MITRE Corporation, n.d.). However, internet trolls quickly realized that they could influence the Chatbot’s learning dataset by posting inappropriate comments, poisoning the dataset that the machine learning model trained from. In less than a day, the product had to be taken down as its tweets had become racist and offensive (MITRE Corporation, n.d.). This real life example demonstrates that poisoning attacks are not academically bound, but a threat that can be taken advantage of in a deployed software product. . Current Industry Stance . Current literature on adversarial machine learning suggests that under ideal conditions, these algorithms can potentially fall prey to adversaries with malicious intent, but the perspective from software companies garners more mixed opinions. While tech giants such as Google, Microsoft, and IBM have pushed forward plans to protect their machine learning use cases, many companies that use machine learning have unclear or no commitment at all to defend against adversarial attacks (Kumar et al., 2020). In a survey paper conducted by Microsoft, 28 organizations were interviewed about their perspectives on machine learning security, out of which only three actively secure their ML systems, and six are ready to assign manpower to the task (Kumar et al., 2020). These interviews demonstrated to the researchers that ML engineers are ill-equipped to deal with adversarial attacks, with reasons ranging from overtrust in current ML frameworks, limited understanding of ML in general, and not knowing what to expect from adversarial attacks (Kumar et al., 2020). Regardless of if adversarial attacks can be successfully used to manipulate machine learning models, this research survey suggests that most machine learning engineers in the software industry are not prepared for the potential threat against their models. . While adversarial machine learning research seems to put priority on the exposure of machine learning models, larger software companies such as Google and Microsoft have begun creating detailed guidelines for good machine learning practices. Microsoft’s AI and ML security documentation includes frameworks for engineers to assess the possibility of adversarial attack while also providing them with detailed plans to combat such attacks (Marshall et al., 2018). Google’s page for Responsible AI Practices also lists recommended practices for ensuring secure models, and encouraging adversarial testing to identify areas where maliciously crafted inputs can harm model outputs (Google, n.d.). While both documents provide concrete steps to follow in ensuring model security from adversarial attacks, both documents also acknowledge the ever changing nature of machine learning, and emphasize that the defense against adversarial attacks still has additional areas of research and growth. These documents indicate that to those prepared, adversarial attacks are not threats with no countermeasures, but a possibility that requires diligent research and preparation. . These large companies may be beginning to treat adversarial machine learning as a real threat, but a machine learning researcher of Google Brain gives another point of view from a software perspective. Nicholas Frosst recognizes the current research on adversarial machine learning, but also suggests that the results from such may be due to the natural behavior of modern machine learning, and that there are much easier ways to fool a model (Synced, 2019). Frosst believes that these models are not designed to make correct predictions on every possible kind of input, and creating perturbed images to fool self-driving cars is much harder than physically removing the stop sign the algorithm is trying to analyze (Synced, 2019). Academic researchers have certainly picked at the topic of adversarial machine learning both theoretically and experimentally, but many seem to agree that the field still has a lot to be explored before it is labeled as a legitimate threat to the software industry. . Conclusion . Machine learning is a field that is growing at a rapid pace, which means adversarial machine learning, or maliciously attacking machine learning models, will evolve at a similarly rapid speed. Even in the last couple of years, there have been hundreds of papers that have exposed weaknesses in machine learning models, using both theoretical and experimental arguments. Academia is pushing the exploration of adversarial attacks as a real threat, but it does not appear to be academically bounded. There have also been cases of adversarial attacks successfully infiltrating models in the software industry, such as Tesla’s autopilot and Microsoft’s Tay Chatbot. A research survey on companies with significant usage of machine learning shows that while engineers seem to agree on the principle of facing adversarial machine learning, most are not equipped with the knowledge to deal with such attacks, and some question the threat they even pose. Adversarial machine learning may be part of an ever growing and ever evolving field, but tech giants like Google and Microsoft show that with the mindset and willingness to embrace academic research with new guidelines, machine learning algorithms can potentially be safe from adversarial attacks. . References . Goodfellow, I., Clark, J., Abbeel, P., Duan, R., Huang, S., &amp; Papernot, N. (2017, February 24). Attacking Machine Learning with Adversarial Examples. OpenAI. Retrieved October 2, 2021, from https://openai.com/blog/adversarial-example-research/. . Google. (n.d.). Responsible AI practices. Google AI. Retrieved October 2, 2021, from https://ai.google/responsibilities/responsible-ai-practices/. . Kumar R. S. S., Nyström M., Lambert J., Marshall A., Goertzel M., Comissoneru A., Swann M., &amp; Xia S. (2020, May). Adversarial machine learning-industry perspectives. In 2020 IEEE Security and Privacy Workshops (SPW) (pp. 69-75). IEEE. . Marshall, A., Rojas, R., Stokes, J., &amp; Brinkman, D. (2018, December 4). Securing the Future of Artificial Intelligence and Machine Learning at Microsoft. Microsoft Docs. Retrieved October 2, 2021, from https://docs.microsoft.com/en-us/security/engineering/securing-artificial-intelligence-machine-learning. . MITRE Corporation. (n.d.). Tay Poisoning. MITRE. Retrieved October 2, 2021, from https://atlas.mitre.org/studies/AML.CS0009/. . Sharif, M., Bhagavatula, S., Bauer, L., &amp; Reiter, M. K. (2017). Adversarial generative nets: Neural network attacks on state-of-the-art face recognition. arXiv preprint arXiv:1801.00349, 2(3). . Synced. (2019, November 21). Google Brain’s Nicholas Frosst on Adversarial Examples and Emotional Responses. Synced. Retrieved October 7, 2021, from https://syncedreview.com/2019/11/21/google-brains-nicholas-frosst-on-adversarial-examples-and-emotional-responses/. . Tencent Keen Security Lab. (2019). Experimental Security Research of Tesla Autopilot [White paper]. Tencent. https://keenlab.tencent.com/en/whitepapers/Experimental_Security_Research_of_Tesla_Autopilot.pdf . Wang, Y., &amp; Chaudhuri, K. (2018). Data poisoning attacks against online learning. arXiv preprint arXiv:1808.08994. .",
            "url": "https://stanleykywu.github.io/ds-blog/2021/10/08/The-Potential-Threat-of-Adversarial-Machine-Learning-in-the-Software-Industry.html",
            "relUrl": "/2021/10/08/The-Potential-Threat-of-Adversarial-Machine-Learning-in-the-Software-Industry.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Stocks, Predictable or Unpredictable?",
            "content": "Usually I would follow what every english instructor in my life has told me: lead with a concise intro. But being the rebellious compsci major I am who has tried to escape all things english related (and failed miserably), I’m going to do something completely cliché to begin my first blog post and write a traditional introduction (thank you english teachers for the many years of practice). This is about a project that a group of friends and I did for our first “real” artificial intelligence class of college. To the fellow NEU students out there, the class is CS4100 and if he’s still teaching, I highly recommend Professor Kevin Gold. . . Is the Stock Market Predictable? . Seems like a relatively straight forward question, and for anyone who’s even remotely breathed in the direction of data science, this is (unsurprisingly) something a lot of people have attempted to answer. But with the latest developments in deep learning, this seems to be something that is potentially in reach. But there’s something different about predicting the stock market, vs. classifying images of cars and bikes for example. In the latter, image classification is a problem that we have manually defined as a challenge for machine learning. We have carefully and manually curated a dataset of images specifically for the purpose of image classifications. And while there is absolutely nothing wrong with that (image classification has become an incredible powerful and important tool of the present), it makes predicting the stock market rather hard to put into … should I say arrays? Maybe at first glance this might seem easy, and it could be. There’s a lot of numbers in stock market information, so shouldn’t that be enough? Well, while the numbers themselves offer some sort of reflection of the stock’s current state, it is so compressed and filtered value that one can argue it offers no value at all whatsoever. Something like news sentiment (whether or not the news that day was positive/negative) could offer a better insight into a stock market price’s trend. Regardless, there’s a massive amount of precious data out there that can potentially help predict stock market prices. But, Stanley, what are you trying to say with all of this? Fine, I’ll oblige: This blog is here to present how we were able to effectively predict Google’s stock market using three simple algorithms, albeit on a very small scale . . Alphabet Inc. Class C . In total, Kaggle was able to give us around 500 days worth of Google’s stock market price data that overlapped with news sentiment data we were able to pull from OpenML. That’s already a pretty small dataset, but we also maintained a roughly 60/20/20 train/val/test split, leaving us with roughly 300 consecutive training days, followed by 100 validation and 100 testing days. Even with this limited data, we were able to make some rather incredible discoveries. . . Maybe Stocks are Easy? . We decided to go with three different machine learning algorithms, a classic LSTM (Long Short-term Memory), a Regression Decision Tree using Residual Squared Sums, and an HMM (Hidden Markov Model). The LSTM represented a standard time series forecasting RNN, while the decision tree and HMM were attempts to apply slighly different algorithms to the task of stock market prediction. Both the LSTM and decision tree were trained on data with a “look-back” of one day. That means the prediction of stock prices for the next day was mainly based on that of the previous day. We found that this approach gave us the best validation performance. The results on testing data are as followed: . . . . What do We Make of this? . Immediately we can see that despite this lack of data, our three algorithms were able to do rather well on data it has never seen before. LSTM gives us consistently solid predictions, with a slight time lag that is consistent with the byproduct of LSTM’s “look-back” training data set. While at first glance, this seems moot, it is very important since day to day stock trading relies on accurate predictions for each specific day. On the other hand, decision trees seem to predict trends slightly before they occur (this is exciting!) due to its blockiness, likely a product of averaging done in the decision tree regressor. Finally for HMM, we can note that while the general trend seems similar, the HMM output tends to overshoot the actual value. This can be attributed to the fact that in the HMM model we used, the closing price tends to skew our predictions higher, which also results in a higher middle price (the value we are matching our predictions to). There also doesn’t seem to be an improved effectiveness of using news sentiment vs. not. Our best guess is that since we are looking at such a small scale (just about a year and half worth of data), that the stock market numbers are more than adequate to make one day decisions on a stock’s overall trend. . . Predicting the stock market may seem like a task perfectly designed for modern day heavy duty neural networks. Yet we show here that in smaller scales with publicly accessible data, a simpler approach may be more than adequate. While these algorithms probably won’t make you Jeff Bezos, it certainly shows that the stock market may not be the unpredictable monster that we used to think. For a more in depth read into this project, feel free to check out the repository on GitHub. .",
            "url": "https://stanleykywu.github.io/ds-blog/machine%20learning/stock%20market/decision%20trees/lstm/hmm/2020/12/14/stonks.html",
            "relUrl": "/machine%20learning/stock%20market/decision%20trees/lstm/hmm/2020/12/14/stonks.html",
            "date": " • Dec 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". More about me . I’m a current Computer Science major @ Northeastern University with a concentration in artificial intelligence. I aspire to be a machine learning researcher or data scientist, and I’m actively involved in adversarial machine learning research. . . Resume_Stanley_Wu.pdf . Hobbies: . Badminton | Classical Music | Boston Sports (Go Red Sox and Patriots!) | Hiking | Discussing Academic Papers (An Image is Worth 16x16 Words anyone?) | Video Games | . While I was writing this, I thought it might be cool to write like a conversation. That way, it’s more fun for me to write and hopefully more fun for you to read! Diving right into my hobbies: I love badminton, been playing since I was a kid (subtle flex but I was once top 20 in the Under 17 Category for Singles in the USA!). Here’s a picture of me at a college badminton tournament. . . I also played the violin in high school, where I got very interested in classical music. Nothing beats playing with an orchestra at Symphony Hall! Some of my favorite composers are Mahler, Beethoven, and Dvorak. Recently, I’ve also realized just how fun and amazing hiking is. Below is a picture of me at Mount Monadnock! . .",
          "url": "https://stanleykywu.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stanleykywu.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}